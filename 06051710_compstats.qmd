---
title: "Computational Statistics Coursework 2025/2026"
subtitle: 'MSc in Statistics 2025/26, Imperial College London'
author: "06051710"
format:
  html:
    toc: true
    highlight: tango
    self-contained: true
    df-print: paged
  pdf: default
format-links: false
editor: 
  markdown: 
    wrap: 72
---

```{=html}
<style type="text/css">
h1{
  font-size: 24pt;
}
h2{
  font-size: 18pt;
}
body{
  font-size: 12pt;
}
</style>
```

```{r setup, include = FALSE, tidy=TRUE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
include_solutions <- TRUE
```

```{r setup2, include=FALSE, tidy=TRUE}
set.seed(17)
require(rmarkdown)
require(knitr)
require(kableExtra)
# Put any library imports and other preamble here.
```

I, 06051710, certify that this assessed coursework is my own work,
unless otherwise acknowledged, and includes no plagiarism. I have not
discussed my coursework with anyone else except when seeking
clarification with the module lecturer via email or on MS Teams. I have
not shared any code underlying my coursework with anyone else prior to
submission.

# Question 1

Throughout this question, I evaluated each sampling algorithm with 1000
samples. I found this made comparing algorithms easier, especially since
the traceplots were less dense. I also discarded the first 5 samples of each algorithm as a burn-in period, as that seemed to be the average number of samples before chains converged to the stationary distribution.

The first thing I did, mainly so that I could get a better idea of where
to initialise my chains from, was plot the target distribution, @fig-1.

```{r}
#| echo: false
#| label: fig-1
#| fig-cap: "Target Distribution up to normalisation constant"

#######################
# Question 1
#######################

library(coda)

target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}

t=seq(0,50, length.out=1000)
plot(t, sapply(t, function(t) target(t)), type="l", xlab="x", ylab = "f(x)")
```

The extremely sharp mode at $x=20$ stood out to me a lot for this
distribution. As such, I decided to initialise the chains from multiple
different starting points (throughout question 1), to more easily view convergence to the true distribution. These
locations were $x=2, 15, 20, 25, 30, 35$, as these were close to the 3
modes around $x$ =2, 20, and 30.

### Part a

When implementing the Random Walk Metropolis Hastings algorithm, I ran
the algorithm separately for standard deviations of 1,3,5,9,11 and 15 in
the proposal distribution, to investigate the impact of different
variances on convergence and mixing quality of my chains.

Visually, I inspected convergence using traceplots in @fig-2 and @fig-3.
As can be seen in @fig-2, for $\sigma$ of 1-5 the chains did not mix
well at all, with no chain sampling from all 3 modes. Convergence seems
best at higher standard deviations; in @fig-3 it is only with a standard
deviation of 11 that it seems most of the modes are sampled from.

```{r}
#| echo: false
#| label: fig-2
#| fig-cap: "Traceplots for smaller standard deviations"


MetrHastw <- function(X0, sigmaprop, nsteps, f){
  X <- numeric(nsteps+1) 
  X[1] <- X0
  for (i in 2:(nsteps+1)){
    Y <- rnorm(1, mean=X[i-1],sd=sigmaprop)
    if (log(runif(1)) <= log(f(Y)) -log(f(X[i-1]))){
      X[i] <- Y}
    else{ X[i] <- X[i-1]}
   }
 X[-(1:5)] #discarding burn in
} 

#seems larger variance is better when we have 10,000 samples for mixing. All chains converge pretty fast, it's just the different modes that make this distribution difficult.

#I should also compute the Gelman Rubin statistic for each chain as a quantitative way to display convergence or lack thereof
#Also group all the plots in one figure

X0s <- c(2, 15, 20, 25, 30, 35)
chains1 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=1, nsteps=1000, f=target)))
chains2 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=3, nsteps=1000, f=target)))
chains3 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=5, nsteps=1000, f=target)))
chains4 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=9, nsteps=1000, f=target)))#actually ends up completely missing the mass around x=20
chains5 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=11, nsteps=1000, f=target)))
chains6 <- lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=15, nsteps=1000, f=target)))

par(mfrow=c(1,3))  

traceplot(chains1); title("sigma = 1")
traceplot(chains2); title("sigma = 3")
traceplot(chains3); title("sigma = 5")
```

```{r}
#| echo: false
#| label: fig-3
#| fig-cap: "Traceplots for larger standard deviations"

par(mfrow=c(1,3))

traceplot(chains4); title("sigma = 9")
traceplot(chains5); title("sigma = 11")
traceplot(chains6); title("sigma = 15")
```

To get a numerical assessment of convergence, I also calculated the
Gelman-Rubin test statistic for each standard deviation.

```{r}
#| echo: false


chains <- list(chains1, chains2, chains3, chains4, chains5, chains6)

res <- data.frame(
  PointEst = sapply(chains, function(x) gelman.diag(x)$psrf[1]),
  UpperCI  = sapply(chains, function(x) gelman.diag(x)$psrf[2])
)

rownames(res) <- c("Sigma 1", "Sigma 3", "Sigma 5", "Sigma 9", "Sigma 11", "Sigma 15")
res
```

Using the common heuristic approach that $\sqrt{\hat{R}} \leq 1.1$ I
find that for standard deviations of 11 and above, the chains likely
converged to the same distribution, however, it seems a standard
deviation of 15 is best, as even the upper confidence interval value
would pass this heuristic test (whereas the same cannot be said for a
standard deviation of 11).

### Part b

The Metropolis-Hasting algorithm I implemented which does not sample
outside the support of $f(x)$ (my target distribution) is an
independence sampler, sampling from a Uniform distribution on \[2,45\].
I chose 45 as the cutoff point for my distribution as there did not seem
to be any probability mass (or anything non negligible) outside of x=45
in my target distribution.

The algorithm works in the following way.

I first initialise my chain with some value
$X_0 \in \{2,15,20,25,30,35\}$, since these are all close to the modes
of my target.

I then repeat the following for $i=1:N$:

1.  Sample $Y \sim U[2,45]$
2.  Accept $Y$ as $X_i$, with probability $min(1,f(Y)/f(X_{i-1}))$ (the
    proposal densities cancel as they are the same for $Y$ and $X_{i-1}$
    since they are on a uniform distribution).
3.  If $Y$ is not accepted, let $X_i = X_{i-1}$

```{r}
#| echo: false
#| label: fig-5
#| fig-cap: "Plots for independence sampler from a Uniform distribution"

### Question 1b

target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}


MH_newprop <- function(X0, nsteps, f){
  X <- numeric(nsteps+1) 
  X[1] <- X0
  
  for (i in 2:(nsteps+1)){
    Y <- runif(1, min=2, max=45)
    log_acc <- log(f(Y)) - log(f(X[i-1])) #the proposal densities cancel out as they are the same everywhere 
    
    if (log(runif(1)) <= log_acc){
      X[i] <- Y
    }
    else{X[i] <- X[i-1]}
   }
 X[-(1:5)]
} 



chains_b <- lapply(X0s, function(x0) mcmc(MH_newprop(x0, nsteps=1000, f=target))) #independence sampler is SUPER good with only 1000 iterations!!!
traceplot(chains_b)



```

As shown in @fig-5, graphically, it seems that the chain has converged
to the true distribution. The Gelman Rubin statistic (below) further
confirms this.

```{r}
#| echo: false
gelman.diag(chains_b)
```

### Part c

The first thing I did to create my parallel tempering algorithm was
define the temperature schedule my distributions would be based on, seen
in @fig-6. Initially, I used the same temperatures as in the solution to
problem set 5.1 in chapter 11 of the notes, however, I increased the final temperatures (from 200 to 400 and 500 to 1500), as I felt the distribution was still rather steep around some modes at lower temperatures.

For the within temperature transition kernels I used a standard Random Walk Metropolis Hastings, however, I used a different standard deviation in the proposal for each temperature. These were higher than in the example and were 0.5,3,6,9 and 12, for temperatures 1-5 respectively. I chose higher standard deviations for higher temperatures as I felt it would help accomplish the goal of exploring the sample space better with different temperature distributions.

For the between distribution transition kernels, I randomly chose two chains next to each other for the swap (with equal probability) and then conducted the swap with probability as specified in Algorithm 3.2 in Chapter 11 of the notes.

In pseudo-code my algorithm worked as follows (with $g$ as the target
distribution):

$$
\begin{aligned}
&\textbf{Repeat for } \text{NUMBEROFSAMPLES:} \\[6pt]
&\quad \textbf{For each chain } m: \\[4pt]
&\qquad \text{Propose } 
    Y = \text{CURRENTSTATE} + N(0, \sigma_m) 
    \quad\text{(RWMH update)} \\[6pt]
&\qquad \text{Accept } Y \text{ as current state if } \\[2pt]
&\qquad\qquad u \sim \mathcal{U}(0,1),\quad
    u \le 
    \frac{g(Y)^{1/T_m}}{g(\text{CURRENTSTATE})^{1/T_m}} \\[12pt]
&\quad \textbf{Swap step:} \\[4pt]
&\quad \text{Sample } m \in \{1,\dots,\text{NUMCHAINS}-1\} \\[4pt]
&\quad \text{For most recent states in chain } m \text{ and chain } m+1: \\[4pt]
&\qquad \text{Swap states if } \\[2pt]
&\qquad\qquad u \sim \mathcal{U}(0,1),\quad
u \le 
\frac{
    g(\text{STATE}_m)^{1/T_{m+1}} \, g(\text{STATE}_{m+1})^{1/T_m}
}{
    g(\text{STATE}_m)^{1/T_m} \, g(\text{STATE}_{m+1})^{1/T_{m+1}}
}
\end{aligned}
$$



```{r}
#| echo: false
#| label: fig-6
#| fig-cap: "Tempered probability distributions, comparing T=1 (pink), T=20 (orange), T=100 (green), T=200 (blue), T=1000 (purple)"

### Question 1c
paralleltemp <- function(num.steps, Ts, sigmas, g, initial) { #taken from chapter 11 problem set 5.1 solution
  M <- length(Ts)
  samples <- matrix(NA, nrow = num.steps, ncol=M)
  
  X <- rep(initial, M) #we start at 3 since we know the target has non zero probability here
  
  for (n in 1:num.steps) {
      # we are going to do a x-step
      for (m in 1:M) {
        y <- X[m] + sigmas[m]*rnorm(1)
        
        #RW Metropolis Hasting Acceptance probability
        alpha <- g(y)^(1/Ts[m]) / g(X[m])^(1/Ts[m])
        
        if (runif(1) <= alpha) {
          X[m] <- y
        }
      }
      ### we are going for a swap
      m <- sample(1:(M-1), 1)
      
      num = (g(X[m])^(1/Ts[m+1])*g(X[m+1])^(1/Ts[m]))
      denom = (g(X[m])^(1/Ts[m])*g(X[m+1])^(1/Ts[m+1]))
      
      if (runif(1) <= num/denom) {
        X[c(m, m+1)] = X[c(m+1, m)]
      }
    
    samples[n, ] = X
  }
  
  return(samples)
}

targetvector <- function(x) {
  ifelse(
    x >= 2,
    exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01),
    0
  )
}

Ts <- c(1,20,100,400,1500) 
#can plot the tempered distributions

x <- seq(0, 50, length.out = 500)

# empty plot
plot(NA, xlim = c(0, 50), ylim = c(0, 1), 
     xlab = "x", ylab = "density")

# colour palette
cols <- hcl.colors(length(Ts), "Set2")

# add lines
for (i in seq_along(Ts)) {
  T <- Ts[i]
  lines(x, targetvector(x)^(1/T), col = cols[i], lwd = 2)
}
```

```{r}
#| echo: false
#| label: fig-7
#| fig-cap: "Traceplot for parallel tempering algorithm"
num.steps <- 1000 
sigmas <- c(0.5,3,6,9,12)
X0s <- c(2, 15, 20, 25, 30, 35)

justchain1 <- function(x0, n) {
  return(paralleltemp(n, Ts, sigmas, g=target, initial=x0)[,1][-(1:5)])
}

chains_parallel <-lapply(X0s, function(x0) mcmc(justchain1(x0, n=num.steps)))


traceplot(chains_parallel); title("Traceplot for Parallel tempering")
```
@fig-7 suggests the chains are exploring the space correctly. Once again I calculated the Gelman-Rubin test statistic as:

```{r}
#| echo: false
gelman.diag(chains_parallel)
```
which suggested the chains likely all converged to the same distribution.

### Part d

All three algorithms managed to converge to the true distribution, at least after their hyperparameters were altered (seen through traceplots and Gelman-Rubin statistics). To compare the mixing quality of the different algorithms, I found the effective sample size for each algorithm's multiple chains (that were initialised at multiple locations) and then averaged them. For the Random Walk Metropolis Hastings, I let $\sigma = 15$ in the proposal distribution, as that seemed to perform best in part a.

These results suggest the Parallel tempering algorithm mixed the best, as it has the highest ESS, followed by the Independence sampler, and then the RWMH. Perhaps if more tuning was done to the hyperparameters in the RWMH, it could have performed better, however, this demonstrates the effectiveness of the Parallel tempering algorithm for distributions with multiple modes.


```{r}
#| echo: false
average_ESS <- function(chains){
  ess <- rep(0,6)
  for (i in 1:6){
    ess[i] <- effectiveSize((chains[i]))
  }
  return (mean(ess))
}

res <- data.frame(
  Method = c("Random Walk", "Independence sampler", "Parallel tempering"),
  ESS = c(
    average_ESS(chains6),
    average_ESS(chains_b),
    average_ESS(chains_parallel)
  )
)

cat("\nAverage ESS by Method\n")
print(res)
```
To compare computational cost, I used the base R function system.time to compare the speeds of all three algorithms. I found the fastest was the independence sampler, followed closely by the random walk metropolis hastings, with the slowest by far being the parallel tempering algorithm (without even mentioning space requirements). This demonstrated a tradeoff between mixing quality and computational efficiency.

```{r}
#| echo: false

#Speed Tests

system.time({
   lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=15, nsteps=100000, f=target)))
})

system.time({
   lapply(X0s, function(x0) mcmc(MH_newprop(x0, nsteps=100000, f=target)))
})


system.time({justchain12 <- function(x0) {
  return(paralleltemp(num.steps=100000, Ts, sigmas, g=target, initial=x0)[,1][-(1:5)])
}
chains_parallel <-lapply(X0s, function(x0) mcmc(justchain12(x0)))
})
```

### Part e

For $k \in {(a,b,c)}$ a formula for a 95% confidence interval for $\hat{I}_n^{(k)}$, which is the estimator for $E[X]$ (where the distribution is represented by the random variable $X$), is:

$$
\bigl[ \hat{I}_n^{(k)} - 1.96 \sqrt{\frac{1}{n} (\text{Var}(X_0) + 2 \sum_{k=1}^{n} \frac{n-k}{n} \text{Cov}(X_0, X_k))}, \;\;
\hat{I}_n^{(k)} + 1.96 \sqrt{\frac{1}{n} (\text{Var}(X_0) + 2 \sum_{k=1}^{n} \frac{n-k}{n} \text{Cov}(X_0, X_k))} \bigr]
$$
where $X_0,...$ are the samples from the sampler implemented in part $k$ (again, assuming convergence to a stationary distribution past a burn-in period of 5 samples). To estimate this confidence interval I use the sample variance and autocovariances.

Using numerical integration (specifically the integrate function in R), I find $E[X]$ to be around 25.2. 

Then for each sampler, I produced a chain initialised at $x=20$, as I found in previous sections that when initialised at 20, the chains sampled the space well. I calculated the point estimate for the expected value of the distribution by calculating the mean of the samples, and then used the sample variances and autocovariances up to $\hat{\gamma}_{100}$ (cutting off at 100 as I found there was rarely any substantial covariation past that point), to create the following plots; @fig-8, @fig-9, and @fig-10.

These plots basically confirm the conclusions from part d; the Random Walk sampler had larger confidence intervals for its estimate of $E[X]$ compared to both the independence sampler and the parallel tempering algorithms; with the latter two samplers having less of an obvious difference between their estimates.

```{r}
#| echo: false
#| results: "hide"
#1e
#Obtaining E[X] through numerical integration

f<- function(x) {
  return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
}
#splitting the range so we don't miss any of the peaks of the function
k<- integrate(f, 2, 18)$value + integrate(f, 18, 22)$value + integrate(f,22, 100)$value 

f_norm <- function(x) {
  return(f(x)/k)
}

exp_x <- function(x){
  return (x*f_norm(x))
}

integrate(exp_x, 2, 50)$value #25.18418- seems reasonable

```




```{r}
#| echo: false
#| label: fig-8
#| fig-cap: "RWMH Estimate of Expected Value"

#1e
#Going to use chains initialised at x=20, as those seemed to sample the full space quite well in parts a-c

IhatA <- function(n){
  samples<- MetrHastw(20, sigmaprop=15, nsteps=n, f=target)
  xbar <- mean(samples)
  
  acovs <- acf(samples, type="covariance", lag.max=100, plot=FALSE)$acf
  summing <- function(n, a){
    total <- 0
    for (k in 1:n){
      total <- total + ((n-k)/n)*a[k]
    }
    return(total)
  }
  half_width <- 1.96* sqrt(1/n * (var(samples) + 2* summing(100, acovs)))
  
  list(
    mean = xbar,
    lower = xbar - half_width,
    upper = xbar + half_width
  )
}

n_vals <- seq(1000, 100000, by = 1000)

results <- t(sapply(n_vals, IhatA))
results <- as.data.frame(results)

# convert everything except n to numeric
results[] <- lapply(results, as.numeric)

results$n <- n_vals

library(ggplot2)

ggplot(results, aes(x = n)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_line(aes(y = mean), linewidth = 1) +
  geom_hline(yintercept = 25.18418, color = "red", linewidth = 0.5, linetype = "dashed") +
  labs(
    x = "n",
    y = "E[X] estimate"
  ) +
  theme_minimal()
```

```{r}
#| echo: false
#| label: fig-9
#| fig-cap: "Independence Sampler Estimate of Expected Value"

IhatB <- function(n){
  samples<- MH_newprop(X0=20, nsteps=n, f=target)
  xbar <- mean(samples)
  
  acovs <- acf(samples, type="covariance", lag.max=100, plot=FALSE)$acf
  summing <- function(n, a){
    total <- 0
    for (k in 1:n){
      total <- total + ((n-k)/n)*a[k]
    }
    return(total)
  }
  half_width <- 1.96* sqrt(1/n * (var(samples) + 2* summing(100, acovs)))
  
  list(
    mean = xbar,
    lower = xbar - half_width,
    upper = xbar + half_width
  )
}

results <- t(sapply(n_vals, IhatB))
results <- as.data.frame(results)

# convert everything except n to numeric
results[] <- lapply(results, as.numeric)

results$n <- n_vals

ggplot(results, aes(x = n)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_line(aes(y = mean), linewidth = 1) +
  geom_hline(yintercept = 25.18418, color = "red", linewidth = 0.5, linetype = "dashed") +
  labs(
    x = "n",
    y = "E[X] estimate"
  ) +
  theme_minimal()
```

```{r}
#| echo: false
#| label: fig-10
#| fig-cap: "Parallel Tempering Sampler Estimate of Expected Value"

IhatC <- function(n){
  samples<- justchain1(20, n)
  xbar <- mean(samples)
  
  acovs <- acf(samples, type="covariance", lag.max=100, plot=FALSE)$acf
  summing <- function(n, a){
    total <- 0
    for (k in 1:n){
      total <- total + ((n-k)/n)*a[k]
    }
    return(total)
  }
  half_width <- 1.96* sqrt(1/n * (var(samples) + 2* summing(100, acovs)))
  
  list(
    mean = xbar,
    lower = xbar - half_width,
    upper = xbar + half_width
  )
}

results <- t(sapply(n_vals, IhatC))
results <- as.data.frame(results)

# convert everything except n to numeric
results[] <- lapply(results, as.numeric)

results$n <- n_vals

ggplot(results, aes(x = n)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_line(aes(y = mean), linewidth = 1) +
  geom_hline(yintercept = 25.18418, color = "red", linewidth = 0.5, linetype = "dashed") +
  labs(
    x = "n",
    y = "E[X] estimate"
  ) +
  theme_minimal()

```

# Question 2

```{r}

###
#2a

library(MASS)

bost_dat <- Boston
bost_dat <- bost_dat[, c("lstat", "rm", "medv")]

train_idx <- sample(1:nrow(bost_dat), size=0.7*nrow(bost_dat))

train_dat <- bost_dat[train_idx, ]
test_dat <- bost_dat[-train_idx, ]

head(train_dat,10)
```

### Part a

```{r}
#| echo: false

loss <- function(y, q, t){ #y is the actual realisation of the predictors. q is my prediction. t is a quantile level
  if (y>q){
    return (t*(y-q))
  }
  else{
    return ((t-1)*(y-q))
  }
}

risk <- function(predictions, actuals, t){ #l is my loss function. data is a matrix of my predictions vs reality. t is the quantile level.
  # Apply the loss function to each pair
  losses <- mapply(FUN=loss, q=predictions, y=actuals, t=t)
  # Compute the average loss
  average_loss <- mean(losses)
  return (average_loss)
}

tau <- c(0.1,0.5,0.9)

riskfun <- function(par, t){
  B0 <- par[1]
  B1 <- par[2]
  B2 <- par[3]
  
  predictions <- B0 + B1*train_dat$lstat + B2*train_dat$rm
  actuals <- train_dat$medv #i.e. the response
  return (risk(predictions, actuals, t))
}

results <- matrix(0, nrow = length(tau), ncol = 3)

for (i in seq_along(tau)) {
  t <- tau[i]
  opt <- round(optim(par = c(5, -5, 5), fn = riskfun, t = t)$par,2)
  results[i, ] <- opt
}

results #I think using this I should find the empirical loss on the test set 





```






# Code Appendix {#sec-code-appendix}

```{r ref.label=knitr::all_labels()}
#| echo: true
#| eval: false
#| code-fold: true

```
