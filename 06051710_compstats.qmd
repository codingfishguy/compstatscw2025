---
title: "Computational Statistics Coursework 2025/2026"
subtitle: 'MSc in Statistics 2025/26, Imperial College London'
author: "06051710"
format:
  html:
    toc: true
    highlight: tango
    self-contained: true
    df-print: paged
  pdf: default
format-links: false
editor: 
  markdown: 
    wrap: 72
---

```{=html}
<style type="text/css">
h1{
  font-size: 24pt;
}
h2{
  font-size: 18pt;
}
body{
  font-size: 12pt;
}
</style>
```

```{r setup, include = FALSE, tidy=TRUE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
include_solutions <- TRUE
```

```{r setup2, include=FALSE, tidy=TRUE}
set.seed(17)
require(rmarkdown)
require(knitr)
require(kableExtra)
# Put any library imports and other preamble here.
```

I, 06051710, certify that this assessed coursework is my own work,
unless otherwise acknowledged, and includes no plagiarism. I have not
discussed my coursework with anyone else except when seeking
clarification with the module lecturer via email or on MS Teams. I have
not shared any code underlying my coursework with anyone else prior to
submission.

# Question 1

Throughout this question, I evaluated each sampling algorithm with 1000
samples. I found this made comparing algorithms easier, especially since
the traceplots were less dense. I also discarded the first 5 samples of
each algorithm as a burn-in period, as that seemed to be the average
number of samples before chains converged to the stationary
distribution.

The first thing I did, mainly so that I could get a better idea of where
to initialise my chains from, was plot the target distribution, @fig-1.

```{r}
#| echo: false
#| label: fig-1
#| fig-cap: "Target Distribution up to normalisation constant"

#######################
# Question 1
#######################

library(coda)

target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}

t=seq(0,50, length.out=1000)
plot(t, sapply(t, function(t) target(t)), type="l", xlab="x", ylab = "f(x)")
```

The extremely sharp mode at $x=20$ stood out to me a lot for this
distribution. As such, I decided to initialise the chains from multiple
different starting points (throughout question 1), to more easily view
convergence to the true distribution. These locations were
$x=2, 15, 20, 25, 30, 35$, as these were close to the 3 modes around $x$
=2, 20, and 30.

### Part a

When implementing the Random Walk Metropolis Hastings algorithm, I ran
the algorithm separately for standard deviations of 1,3,5,9,11 and 15 in
the proposal distribution, to investigate the impact of different
variances on convergence and mixing quality of my chains.

Visually, I inspected convergence using traceplots in @fig-2 and @fig-3.
As can be seen in @fig-2, for $\sigma$ of 1-5 the chains did not mix
well at all, with no chain sampling from all 3 modes. Convergence seems
best at higher standard deviations; in @fig-3 it is only with a standard
deviation of 11 that it seems most of the modes are sampled from.

```{r}
#| echo: false
#| label: fig-2
#| fig-cap: "Traceplots for smaller standard deviations"


MetrHastw <- function(X0, sigmaprop, nsteps, f){
  X <- numeric(nsteps+1) 
  X[1] <- X0
  for (i in 2:(nsteps+1)){
    Y <- rnorm(1, mean=X[i-1],sd=sigmaprop)
    if (log(runif(1)) <= log(f(Y)) -log(f(X[i-1]))){
      X[i] <- Y}
    else{ X[i] <- X[i-1]}
   }
 X[-(1:5)] #discarding burn in
} 

#seems larger variance is better when we have 10,000 samples for mixing. All chains converge pretty fast, it's just the different modes that make this distribution difficult.

#I should also compute the Gelman Rubin statistic for each chain as a quantitative way to display convergence or lack thereof
#Also group all the plots in one figure

X0s <- c(2, 15, 20, 25, 30, 35)
chains1 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=1, nsteps=1000, f=target)))
chains2 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=3, nsteps=1000, f=target)))
chains3 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=5, nsteps=1000, f=target)))
chains4 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=9, nsteps=1000, f=target)))#actually ends up completely missing the mass around x=20
chains5 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=11, nsteps=1000, f=target)))
chains6 <- lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=15, nsteps=1000, f=target)))

par(mfrow=c(1,3))  

traceplot(chains1); title("sigma = 1")
traceplot(chains2); title("sigma = 3")
traceplot(chains3); title("sigma = 5")
```

```{r}
#| echo: false
#| label: fig-3
#| fig-cap: "Traceplots for larger standard deviations"

par(mfrow=c(1,3))

traceplot(chains4); title("sigma = 9")
traceplot(chains5); title("sigma = 11")
traceplot(chains6); title("sigma = 15")
```

To get a numerical assessment of convergence, I also calculated the
Gelman-Rubin test statistic for each standard deviation.

```{r}
#| echo: false


chains <- list(chains1, chains2, chains3, chains4, chains5, chains6)

res <- data.frame(
  PointEst = sapply(chains, function(x) gelman.diag(x)$psrf[1]),
  UpperCI  = sapply(chains, function(x) gelman.diag(x)$psrf[2])
)

rownames(res) <- c("Sigma 1", "Sigma 3", "Sigma 5", "Sigma 9", "Sigma 11", "Sigma 15")
res
```

Using the common heuristic approach that $\sqrt{\hat{R}} \leq 1.1$ I
find that for standard deviations of 11 and above, the chains likely
converged to the same distribution, however, it seems a standard
deviation of 15 is best, as even the upper confidence interval value
would pass this heuristic test (whereas the same cannot be said for a
standard deviation of 11).

### Part b

The Metropolis-Hasting algorithm I implemented which does not sample
outside the support of $f(x)$ (my target distribution) is an
independence sampler, sampling from a Uniform distribution on \[2,45\].
I chose 45 as the cutoff point for my distribution as there did not seem
to be any probability mass (or anything non negligible) outside of x=45
in my target distribution.

The algorithm works in the following way.

I first initialise my chain with some value
$X_0 \in \{2,15,20,25,30,35\}$, since these are all close to the modes
of my target.

I then repeat the following for $i=1:N$:

1.  Sample $Y \sim U[2,45]$
2.  Accept $Y$ as $X_i$, with probability $min(1,f(Y)/f(X_{i-1}))$ (the
    proposal densities cancel as they are the same for $Y$ and $X_{i-1}$
    since they are on a uniform distribution).
3.  If $Y$ is not accepted, let $X_i = X_{i-1}$

```{r}
#| echo: false
#| label: fig-5
#| fig-cap: "Plots for independence sampler from a Uniform distribution"

### Question 1b

target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}


MH_newprop <- function(X0, nsteps, f){
  X <- numeric(nsteps+1) 
  X[1] <- X0
  
  for (i in 2:(nsteps+1)){
    Y <- runif(1, min=2, max=45)
    log_acc <- log(f(Y)) - log(f(X[i-1])) #the proposal densities cancel out as they are the same everywhere 
    
    if (log(runif(1)) <= log_acc){
      X[i] <- Y
    }
    else{X[i] <- X[i-1]}
   }
 X[-(1:5)]
} 



chains_b <- lapply(X0s, function(x0) mcmc(MH_newprop(x0, nsteps=1000, f=target))) #independence sampler is SUPER good with only 1000 iterations!!!
traceplot(chains_b)



```

As shown in @fig-5, graphically, it seems that the chain has converged
to the true distribution. The Gelman Rubin statistic (below) further
confirms this.

```{r}
#| echo: false
gelman.diag(chains_b)
```

### Part c

The first thing I did to create my parallel tempering algorithm was
define the temperature schedule my distributions would be based on, seen
in @fig-6. Initially, I used the same temperatures as in the solution to
problem set 5.1 in chapter 11 of the notes, however, I increased the
final temperatures (from 200 to 400 and 500 to 1500), as I felt the
distribution was still rather steep around some modes at lower
temperatures.

For the within temperature transition kernels I used a standard Random
Walk Metropolis Hastings, however, I used a different standard deviation
in the proposal for each temperature. These were higher than in the
example and were 0.5,3,6,9 and 12, for temperatures 1-5 respectively. I
chose higher standard deviations for higher temperatures as I felt it
would help accomplish the goal of exploring the sample space better with
different temperature distributions.

For the between distribution transition kernels, I randomly chose two
chains next to each other for the swap (with equal probability) and then
conducted the swap with probability as specified in Algorithm 3.2 in
Chapter 11 of the notes.

In pseudo-code my algorithm worked as follows (with $g$ as the target
distribution):

$$
\begin{aligned}
&\textbf{Repeat for } \text{NUMBEROFSAMPLES:} \\[6pt]
&\quad \textbf{For each chain } m: \\[4pt]
&\qquad \text{Propose } 
    Y = \text{CURRENTSTATE} + N(0, \sigma_m) 
    \quad\text{(RWMH update)} \\[6pt]
&\qquad \text{Accept } Y \text{ as current state if } \\[2pt]
&\qquad\qquad u \sim \mathcal{U}(0,1),\quad
    u \le 
    \frac{g(Y)^{1/T_m}}{g(\text{CURRENTSTATE})^{1/T_m}} \\[12pt]
&\quad \textbf{Swap step:} \\[4pt]
&\quad \text{Sample } m \in \{1,\dots,\text{NUMCHAINS}-1\} \\[4pt]
&\quad \text{For most recent states in chain } m \text{ and chain } m+1: \\[4pt]
&\qquad \text{Swap states if } \\[2pt]
&\qquad\qquad u \sim \mathcal{U}(0,1),\quad
u \le 
\frac{
    g(\text{STATE}_m)^{1/T_{m+1}} \, g(\text{STATE}_{m+1})^{1/T_m}
}{
    g(\text{STATE}_m)^{1/T_m} \, g(\text{STATE}_{m+1})^{1/T_{m+1}}
}
\end{aligned}
$$

```{r}
#| echo: false
#| label: fig-6
#| fig-cap: "Tempered probability distributions, comparing T=1 (pink), T=20 (orange), T=100 (green), T=200 (blue), T=1000 (purple)"

### Question 1c
paralleltemp <- function(num.steps, Ts, sigmas, g, initial) { #taken from chapter 11 problem set 5.1 solution
  M <- length(Ts)
  samples <- matrix(NA, nrow = num.steps, ncol=M)
  
  X <- rep(initial, M) #we start at 3 since we know the target has non zero probability here
  
  for (n in 1:num.steps) {
      # we are going to do a x-step
      for (m in 1:M) {
        y <- X[m] + sigmas[m]*rnorm(1)
        
        #RW Metropolis Hasting Acceptance probability
        alpha <- g(y)^(1/Ts[m]) / g(X[m])^(1/Ts[m])
        
        if (runif(1) <= alpha) {
          X[m] <- y
        }
      }
      ### we are going for a swap
      m <- sample(1:(M-1), 1)
      
      num = (g(X[m])^(1/Ts[m+1])*g(X[m+1])^(1/Ts[m]))
      denom = (g(X[m])^(1/Ts[m])*g(X[m+1])^(1/Ts[m+1]))
      
      if (runif(1) <= num/denom) {
        X[c(m, m+1)] = X[c(m+1, m)]
      }
    
    samples[n, ] = X
  }
  
  return(samples)
}

targetvector <- function(x) {
  ifelse(
    x >= 2,
    exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01),
    0
  )
}

Ts <- c(1,20,100,400,1500) 
#can plot the tempered distributions

x <- seq(0, 50, length.out = 500)

# empty plot
plot(NA, xlim = c(0, 50), ylim = c(0, 1), 
     xlab = "x", ylab = "density")

# colour palette
cols <- hcl.colors(length(Ts), "Set2")

# add lines
for (i in seq_along(Ts)) {
  T <- Ts[i]
  lines(x, targetvector(x)^(1/T), col = cols[i], lwd = 2)
}
```

```{r}
#| echo: false
#| label: fig-7
#| fig-cap: "Traceplot for parallel tempering algorithm"
num.steps <- 1000 
sigmas <- c(0.5,3,6,9,12)
X0s <- c(2, 15, 20, 25, 30, 35)

justchain1 <- function(x0, n) {
  return(paralleltemp(n, Ts, sigmas, g=target, initial=x0)[,1][-(1:5)])
}

chains_parallel <-lapply(X0s, function(x0) mcmc(justchain1(x0, n=num.steps)))


traceplot(chains_parallel); title("Traceplot for Parallel tempering")
```

@fig-7 suggests the chains are exploring the space correctly. Once again
I calculated the Gelman-Rubin test statistic as:

```{r}
#| echo: false
gelman.diag(chains_parallel)
```

which suggested the chains likely all converged to the same
distribution.

### Part d

All three algorithms managed to converge to the true distribution, at
least after their hyperparameters were altered (seen through traceplots
and Gelman-Rubin statistics). To compare the mixing quality of the
different algorithms, I found the effective sample size for each
algorithm's multiple chains (that were initialised at multiple
locations) and then averaged them. For the Random Walk Metropolis
Hastings, I let $\sigma = 15$ in the proposal distribution, as that
seemed to perform best in part a.

These results suggest the Parallel tempering algorithm mixed the best,
as it has the highest ESS, followed by the Independence sampler, and
then the RWMH. Perhaps if more tuning was done to the hyperparameters in
the RWMH, it could have performed better, however, this demonstrates the
effectiveness of the Parallel tempering algorithm for distributions with
multiple modes.

```{r}
#| echo: false
average_ESS <- function(chains){
  ess <- rep(0,6)
  for (i in 1:6){
    ess[i] <- effectiveSize((chains[i]))
  }
  return (mean(ess))
}

res <- data.frame(
  Method = c("Random Walk", "Independence sampler", "Parallel tempering"),
  ESS = c(
    average_ESS(chains6),
    average_ESS(chains_b),
    average_ESS(chains_parallel)
  )
)

cat("\nAverage ESS by Method\n")
print(res)
```

To compare computational cost, I used the base R function system.time to
compare the speeds of all three algorithms. I found the fastest was the
independence sampler, followed closely by the random walk metropolis
hastings, with the slowest by far being the parallel tempering algorithm
(without even mentioning space requirements). This demonstrated a
tradeoff between mixing quality and computational efficiency.

```{r}
#| echo: false

#Speed Tests

system.time({
   lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=15, nsteps=100000, f=target)))
})

system.time({
   lapply(X0s, function(x0) mcmc(MH_newprop(x0, nsteps=100000, f=target)))
})


system.time({justchain12 <- function(x0) {
  return(paralleltemp(num.steps=100000, Ts, sigmas, g=target, initial=x0)[,1][-(1:5)])
}
chains_parallel <-lapply(X0s, function(x0) mcmc(justchain12(x0)))
})
```

### Part e

For $k \in {(a,b,c)}$ a formula for a 95% confidence interval for
$\hat{I}_n^{(k)}$, which is the estimator for $E[X]$ (where the
distribution is represented by the random variable $X$), is:

$$
\bigl[ \hat{I}_n^{(k)} - 1.96 \sqrt{\frac{1}{n} (\text{Var}(X_0) + 2 \sum_{k=1}^{n} \frac{n-k}{n} \text{Cov}(X_0, X_k))}, \;\;
\hat{I}_n^{(k)} + 1.96 \sqrt{\frac{1}{n} (\text{Var}(X_0) + 2 \sum_{k=1}^{n} \frac{n-k}{n} \text{Cov}(X_0, X_k))} \bigr]
$$ where $X_0,...$ are the samples from the sampler implemented in part
$k$ (again, assuming convergence to a stationary distribution past a
burn-in period of 5 samples). To estimate this confidence interval I use
the sample variance and autocovariances.

Using numerical integration (specifically the integrate function in R),
I find $E[X]$ to be around 25.2.

Then for each sampler, I produced a chain initialised at $x=20$, as I
found in previous sections that when initialised at 20, the chains
sampled the space well. I calculated the point estimate for the expected
value of the distribution by calculating the mean of the samples, and
then used the sample variances and autocovariances up to
$\hat{\gamma}_{100}$ (cutting off at 100 as I found there was rarely any
substantial covariation past that point), to create the following plots;
@fig-8, @fig-9, and @fig-10.

These plots basically confirm the conclusions from part d; the Random
Walk sampler had larger confidence intervals for its estimate of $E[X]$
compared to both the independence sampler and the parallel tempering
algorithms; with the latter two samplers having less of an obvious
difference between their estimates.

```{r}
#| echo: false
#| results: "hide"
#1e
#Obtaining E[X] through numerical integration

f<- function(x) {
  return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
}
#splitting the range so we don't miss any of the peaks of the function
k<- integrate(f, 2, 18)$value + integrate(f, 18, 22)$value + integrate(f,22, 100)$value 

f_norm <- function(x) {
  return(f(x)/k)
}

exp_x <- function(x){
  return (x*f_norm(x))
}

integrate(exp_x, 2, 50)$value #25.18418- seems reasonable

```

```{r}
#| echo: false
#| label: fig-8
#| fig-cap: "RWMH Estimate of Expected Value"

#1e
#Going to use chains initialised at x=20, as those seemed to sample the full space quite well in parts a-c

IhatA <- function(n){
  samples<- MetrHastw(20, sigmaprop=15, nsteps=n, f=target)
  xbar <- mean(samples)
  
  acovs <- acf(samples, type="covariance", lag.max=100, plot=FALSE)$acf
  summing <- function(n, a){
    total <- 0
    for (k in 1:n){
      total <- total + ((n-k)/n)*a[k]
    }
    return(total)
  }
  half_width <- 1.96* sqrt(1/n * (var(samples) + 2* summing(100, acovs)))
  
  list(
    mean = xbar,
    lower = xbar - half_width,
    upper = xbar + half_width
  )
}

n_vals <- seq(1000, 100000, by = 1000)

results <- t(sapply(n_vals, IhatA))
results <- as.data.frame(results)

# convert everything except n to numeric
results[] <- lapply(results, as.numeric)

results$n <- n_vals

library(ggplot2)

ggplot(results, aes(x = n)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_line(aes(y = mean), linewidth = 1) +
  geom_hline(yintercept = 25.18418, color = "red", linewidth = 0.5, linetype = "dashed") +
  labs(
    x = "n",
    y = "E[X] estimate"
  ) +
  theme_minimal()
```

```{r}
#| echo: false
#| label: fig-9
#| fig-cap: "Independence Sampler Estimate of Expected Value"

IhatB <- function(n){
  samples<- MH_newprop(X0=20, nsteps=n, f=target)
  xbar <- mean(samples)
  
  acovs <- acf(samples, type="covariance", lag.max=100, plot=FALSE)$acf
  summing <- function(n, a){
    total <- 0
    for (k in 1:n){
      total <- total + ((n-k)/n)*a[k]
    }
    return(total)
  }
  half_width <- 1.96* sqrt(1/n * (var(samples) + 2* summing(100, acovs)))
  
  list(
    mean = xbar,
    lower = xbar - half_width,
    upper = xbar + half_width
  )
}

results <- t(sapply(n_vals, IhatB))
results <- as.data.frame(results)

# convert everything except n to numeric
results[] <- lapply(results, as.numeric)

results$n <- n_vals

ggplot(results, aes(x = n)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_line(aes(y = mean), linewidth = 1) +
  geom_hline(yintercept = 25.18418, color = "red", linewidth = 0.5, linetype = "dashed") +
  labs(
    x = "n",
    y = "E[X] estimate"
  ) +
  theme_minimal()
```

```{r}
#| echo: false
#| label: fig-10
#| fig-cap: "Parallel Tempering Sampler Estimate of Expected Value"

IhatC <- function(n){
  samples<- justchain1(20, n)
  xbar <- mean(samples)
  
  acovs <- acf(samples, type="covariance", lag.max=100, plot=FALSE)$acf
  summing <- function(n, a){
    total <- 0
    for (k in 1:n){
      total <- total + ((n-k)/n)*a[k]
    }
    return(total)
  }
  half_width <- 1.96* sqrt(1/n * (var(samples) + 2* summing(100, acovs)))
  
  list(
    mean = xbar,
    lower = xbar - half_width,
    upper = xbar + half_width
  )
}

results <- t(sapply(n_vals, IhatC))
results <- as.data.frame(results)

# convert everything except n to numeric
results[] <- lapply(results, as.numeric)

results$n <- n_vals

ggplot(results, aes(x = n)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_line(aes(y = mean), linewidth = 1) +
  geom_hline(yintercept = 25.18418, color = "red", linewidth = 0.5, linetype = "dashed") +
  labs(
    x = "n",
    y = "E[X] estimate"
  ) +
  theme_minimal()

```

# Question 2

```{r}
#| echo: false
###
#2a

library(MASS)

bost_dat <- Boston
bost_dat <- bost_dat[, c("lstat", "rm", "medv")]

train_idx <- sample(1:nrow(bost_dat), size=0.7*nrow(bost_dat))

train_dat <- bost_dat[train_idx, ]
test_dat <- bost_dat[-train_idx, ]

rownames(train_dat) <- NULL
rownames(test_dat) <- NULL
```

### Part a

I implemented the empirical risk function as a function of the model parameters in a linear model ($\beta_0$, $\beta_1$, $\beta_2$), my dataset, and desired quantile level. The function generated predictions through a baseline linear model (with parameters as specified), using lstat and rm in the dataset as predictors, and then the average loss of these predictions compared to the
actual realisation of the data (medv) was computed.

I then used the optim function in R to minimise this empirical risk function over the training data for $\tau = (0.1,0.5,0.9)$. For the initial state for my parameters $\beta_0$, $\beta_1$, $\beta_2$
I chose $(0,-5,5)$. I chose a negative value for the coefficient of lstat as I felt it made
sense lower socioeconomic status would correlate with lower house prices
and similarly felt the number of rooms in a house would correlate with
higher house prices.I assessed convergence using the convergence part of the
optim function; as can be seen, it was 0 for all values of $\tau$,
suggesting successful convergence. All other arguments in the optim function were left as default.

This led to the following estimates for my parameters:

```{r}
#| echo: false

loss <- function(y, q, t){ #y is the actual realisation of the predictors. q is my prediction. t is a quantile level
  if (y>q){
    return (t*(y-q))
  }
  else{
    return ((t-1)*(y-q))
  }
}

risk_linear <- function(par, data, t){
  B0 <- par[1]
  B1 <- par[2]
  B2 <- par[3]
  
  predictions <- B0 + B1*data$lstat + B2*data$rm
  actuals <- data$medv
  losses <- mapply(FUN=loss, q=predictions, y=actuals, t=t)
  return (mean(losses))
}

tau <- c(0.1,0.5,0.9)
results_linear <- matrix(0, nrow = length(tau), ncol = 4)

for (i in seq_along(tau)) {
  t <- tau[i]
  opt <- optim(par = c(0, -5, 5), fn = risk_linear, data=train_dat, t = t)
  results_linear[i, ] <- c(round(opt$par, 2), opt$convergence)
}

colnames(results_linear) <- c("B0", "B1", "B2", "Convergence")
rownames(results_linear) <- paste0("tau_", tau)

# convert to data frame for nicer printing
results_df <- as.data.frame(results_linear)
results_df[] <- lapply(results_df, function(x) if(is.numeric(x)) format(x, nsmall = 2) else x)

cat("Parameter Fits for Baseline Linear Model\n")
print(results_df, row.names = TRUE, right = TRUE)

```

The Empirical Loss was then calculated by summing the loss between all the predictions and actual values of the data, calculated on the test set.

```{r}
#| echo: false

#Empirical Loss on Test Set

losses <- c(
  risk_linear(c(results_linear[1,1], results_linear[1,2], results_linear[1,3]), t=0.1, data=test_dat)*nrow(test_dat),
  risk_linear(c(results_linear[2,1], results_linear[2,2], results_linear[2,3]), t=0.5, data=test_dat)*nrow(test_dat),
  risk_linear(c(results_linear[3,1], results_linear[3,2], results_linear[3,3]), t=0.9, data=test_dat)*nrow(test_dat)
)

df <- data.frame(`Empirical Loss` = losses, 
                 row.names = c("tau_0.1", "tau_0.5", "tau_0.9"))
df
```

Finally, I calculated empirical coverage by once again using the same coefficients fitted on the training data, and creating a function to find empirical coverage. As can be seen, the empirical coverage was rather close to each desired $\tau$, except for the median, which seemed a little underestimated.

```{r}
#| echo: false

#Empirical Coverage

cov <- function(predictions, actuals){
   return (mean(actuals<=predictions))
}

emp_cov <- function(par){
  B0 <- par[1]
  B1 <- par[2]
  B2 <- par[3]
  
  predictions <- B0 + B1*test_dat$lstat + B2*test_dat$rm
  actuals <- test_dat$medv #i.e. the response
  return (cov(predictions, actuals))
}

coverages <- c(emp_cov(c(results_linear[1,1], results_linear[1,2], 
                         results_linear[1,3])), 
               emp_cov(c(results_linear[2,1], results_linear[2,2], 
                         results_linear[2,3])),
               emp_cov(c(results_linear[3,1], results_linear[3,2], 
                         results_linear[3,3]))
              )


df <- data.frame(`Empirical Coverage` = coverages, 
                 row.names = c("tau_0.1", "tau_0.5", "tau_0.9"))
df

```

### Part b

I fit the second model to the function:

$$
f_{\tau}^{(1)} = \beta_{0,\tau} + \beta_{1,\tau} \text{lstat} + \beta_{2,\tau} \text{rm} + \beta_{3,\tau} \text{lstat}^2 + \beta_{4,\tau} \text{rm}^2 + \beta_{5,\tau} \text{lstat} \cdot \text{rm}
$$
I created my risk function to minimise based on the predictions generated by this model, and then minimised it with optim. However, instead of using the default optimising method again, which I found did not lead to convergence, I switched my optimiser to BFGS, which was successful in converging to an estimate (confirmed through the convergence part of optim being 0 for all $\tau$). Additionally, I initialised all my parameters to 0, as I was less certain about the impact of different predictors with non-linear terms. All other arguments in the optim function remained as default settings.


```{r}
#| echo: false

#2b

tau <- c(0.1,0.5,0.9)

risk_complex <- function(par, t, data){
  B0 <- par[1]
  B1 <- par[2]
  B2 <- par[3]
  B3 <- par[4]
  B4 <- par[5]
  B5 <- par[6]
  
  predictions <- B0 + B1*data$lstat + B2*data$rm + 
    B3*(data$lstat)^2 + B4 * (data$rm)^2 + 
    B5* (data$rm * data$lstat)
  
  actuals <- data$medv
  losses <- mapply(FUN=loss, q=predictions, y=actuals, t=t)
  return (mean(losses))
}

results_complex <- matrix(0, nrow = length(tau), ncol = 7)

for (i in seq_along(tau)) {
  t <- tau[i]
  opt <- optim(par = c(0,0,0,0,0,0), fn = risk_complex, t = t, data=train_dat, 
               method="BFGS")

  ## store parameters + convergence code
  results_complex[i, ] <- c(round(opt$par, 3), opt$convergence)
}

colnames(results_complex) <- c("B0", "B1", "B2", "B3", "B4", "B5", 
                               "Convergence")
rownames(results_complex) <- paste0("tau_", tau)

# convert to data frame for nicer printing
results_df2 <- as.data.frame(results_complex)
results_df2[] <- lapply(results_df2, function(x) 
  if(is.numeric(x)) format(x, nsmall = 2) else x)

cat("Parameter Fits for More Complex Model\n")
print(results_df2, row.names = TRUE, right = TRUE)
```
I then calculated the empirical loss and coverage the same way as in part a. 

```{r}
#| echo: false

#Test Loss and empirical coverage for model 2


# Compute the empirical losses
losses2 <- c(
  risk_complex(c(results_complex[1,1], results_complex[1,2], 
                 results_complex[1,3], results_complex[1,4], 
                 results_complex[1,5], results_complex[1,6]), 
               t=0.1, data=test_dat) *nrow(test_dat),
  risk_complex(c(results_complex[2,1], results_complex[2,2], 
                 results_complex[2,3], results_complex[2,4],
                 results_complex[2,5], results_complex[2,6]), 
               t=0.5, data=test_dat) *nrow(test_dat),
  risk_complex(c(results_complex[3,1], results_complex[3,2], 
                 results_complex[3,3], results_complex[3,4], 
                 results_complex[3,5], results_complex[3,6]), 
               t=0.9, data=test_dat) *nrow(test_dat)
)

# Create the dataframe
df2 <- data.frame(`Empirical Loss` = losses2, 
                 row.names = c("tau_0.1", "tau_0.5", "tau_0.9"))
# Display the dataframe
df2

emp_cov2 <- function(par){
  B0 <- par[1]
  B1 <- par[2]
  B2 <- par[3]
  B3 <- par[4]
  B4 <- par[5]
  B5 <- par[6]
  
  predictions <- B0 + B1*test_dat$lstat + B2*test_dat$rm + 
    B3*(test_dat$lstat)^2 + B4 * (test_dat$rm)^2 + 
    B5* (test_dat$rm * test_dat$lstat)
  actuals <- test_dat$medv #i.e. the response
  return (cov(predictions, actuals))
}



coverages2 <- c(emp_cov2(c(results_complex[1,1], results_complex[1,2], 
                           results_complex[1,3], results_complex[1,4], 
                           results_complex[1,5], results_complex[1,6])),
                emp_cov2(c(results_complex[2,1], results_complex[2,2], 
                           results_complex[2,3], results_complex[2,4], 
                           results_complex[2,5], results_complex[2,6])),
                emp_cov2(c(results_complex[3,1], results_complex[3,2], 
                           results_complex[3,3], results_complex[3,4], 
                           results_complex[3,5], results_complex[3,6]))
)



df3 <- data.frame(`Empirical Coverage` = coverages2, 
                 row.names = c("tau_0.1", "tau_0.5", "tau_0.9"))
df3


```
### Part c

I created the penalised function to minimise by adding a ridge penalty term to the risk function defined in the previous section. I then created 5 folds of the training data and used k folds cross validation to find the loss for different values of the hyperparameter $\lambda$ used in the expression for penalised loss, as well as for the different values of $\tau$, as previously specified. I also included the cross validation loss for $\lambda=0$, which corresponds to a model with no penalisation term. 

```{r}
#| echo: false

risk_ridge <- function(par, t, data, l){
  B0 <- par[1]
  B1 <- par[2]
  B2 <- par[3]
  B3 <- par[4]
  B4 <- par[5]
  B5 <- par[6]
  
  predictions <- B0 + B1*data$lstat + B2*data$rm + 
    B3*(data$lstat)^2 + B4 * (data$rm)^2 + 
    B5* (data$rm * data$lstat)
  
  actuals <- data$medv
  losses <- mapply(FUN=loss, q=predictions, y=actuals, t=t)
  penalised_loss <- mean(losses) + l*sum(par^2)
  return (penalised_loss)
}

kfold <- 5

kfoldcross <- function(tau, lambda){
  fold <- sample(rep(1:kfold, length.out = nrow(train_dat)))
  loss_folds <- rep(100,kfold)
  
  for (j in 1:kfold){
    tr <- train_dat[fold!=j, ]
    va <- train_dat[fold==j, ]
    
    #fit model for given tau
    opt <- optim(par = c(0,0,0,0,0,0), fn = risk_ridge, t = tau, data=tr, 
                 l=lambda, 
                 method="BFGS")
    coeff_ridge <- opt$par
    if (opt$convergence > 0) {return ("DID NOT CONVERGE!")}
    
    loss_folds[j] <- risk_complex(par = coeff_ridge, t=tau, data=va)   
  }
  
  return (mean(loss_folds))
}


taus <- c(0.1, 0.5, 0.9)
lambdas <- c(0.0001, 0.0005, 0.001, 0.005, 0.01, 0)

results <- matrix(NA, nrow = length(taus), ncol = length(lambdas),
                  dimnames = list(paste0("tau=", taus),
                                  paste0("lambda=", round(lambdas, 5))))

for (i in seq_along(taus)) {
  for (j in seq_along(lambdas)) {
    results[i, j] <- kfoldcross(tau = taus[i], lambda = lambdas[j])
  }
}

print("K folds loss for different values of lambda and tau")
print(results)
```
Using the above results, I then fitted a model with the ridge penalty on the full training data, using the values of $\lambda$ that corresponded to the lowest cross validation error (I never used $\lambda=0$, as that was done in part b). The values of $\lambda$ I ended up using were $\lambda = (0.001, 0.0005, 0.0001)$, for $\tau = (0.1,0.5,0.9)$ respectively. This led to the following coefficient estimates:

```{r}
#| echo: false

#Fitting the model for each tau with the best lambda 
#Using full training data
lambdas <- c(0.001, 0.0005, 0.0001)
results_ridge <- matrix(0, nrow=length(tau), ncol=7)

for (i in seq_along(tau)) {
  t <- tau[i]
  opt <- optim(par = c(0,0,0,0,0,0), fn = risk_ridge, t = t, l = lambdas[i], 
               data=train_dat, 
               method="BFGS")
  results_ridge[i, ] <- c(round(opt$par, 3), opt$convergence)
}

colnames(results_ridge) <- c("B0", "B1", "B2", "B3", "B4", "B5", 
                               "Convergence")
rownames(results_ridge) <- paste0("tau_", tau)

# convert to data frame for nicer printing
results_df2 <- as.data.frame(results_ridge)
results_df2[] <- lapply(results_df2, function(x) 
  if(is.numeric(x)) format(x, nsmall = 2) else x)

cat("Parameter Fits for Penalised Model\n")
print(results_df2, row.names = TRUE, right = TRUE)
```
Finally, I found the empirical loss and coverage for the penalised models with coefficients as above in the same ways as part a and b:

```{r}
#| echo: false

#Finding the empirical loss and coverage for ridge penalised model

losses_ridge <- c(
  risk_complex(c(results_ridge[1,1], results_ridge[1,2], 
                 results_ridge[1,3], results_ridge[1,4], 
                 results_ridge[1,5], results_ridge[1,6]), 
               t=0.1, data=test_dat) *nrow(test_dat),
  risk_complex(c(results_ridge[2,1], results_ridge[2,2], 
                 results_ridge[2,3], results_ridge[2,4],
                 results_ridge[2,5], results_ridge[2,6]), 
               t=0.5, data=test_dat) *nrow(test_dat),
  risk_complex(c(results_ridge[3,1], results_ridge[3,2], 
                 results_ridge[3,3], results_ridge[3,4], 
                 results_ridge[3,5], results_ridge[3,6]), 
               t=0.9, data=test_dat) *nrow(test_dat)
)


# Create the dataframe
df2 <- data.frame(`Empirical Loss` = losses_ridge, 
                 row.names = c("tau_0.1", "tau_0.5", "tau_0.9"))
# Display the dataframe
df2

emp_cov2 <- function(par){
  B0 <- par[1]
  B1 <- par[2]
  B2 <- par[3]
  B3 <- par[4]
  B4 <- par[5]
  B5 <- par[6]
  
  predictions <- B0 + B1*test_dat$lstat + B2*test_dat$rm + 
    B3*(test_dat$lstat)^2 + B4 * (test_dat$rm)^2 + 
    B5* (test_dat$rm * test_dat$lstat)
  actuals <- test_dat$medv #i.e. the response
  return (cov(predictions, actuals))
}



coverages_ridge <- c(emp_cov2(c(results_ridge[1,1], results_ridge[1,2], 
                           results_ridge[1,3], results_ridge[1,4], 
                           results_ridge[1,5], results_ridge[1,6])),
                emp_cov2(c(results_ridge[2,1], results_ridge[2,2], 
                           results_ridge[2,3], results_ridge[2,4], 
                           results_ridge[2,5], results_ridge[2,6])),
                emp_cov2(c(results_ridge[3,1], results_ridge[3,2], 
                           results_ridge[3,3], results_ridge[3,4], 
                           results_ridge[3,5], results_ridge[3,6]))
)



df3 <- data.frame(`Empirical Coverage` = coverages_ridge, 
                 row.names = c("tau_0.1", "tau_0.5", "tau_0.9"))
df3



```

### Part d

In terms of coefficient magnitudes my first model fitted the largest coefficients of all three models, with the second model having much smaller coefficients, and the third (the penalised model) having the smallest coefficients. 

The test loss also decreased between the first model and the second model considerably so, with the 








# Code Appendix {#sec-code-appendix}

```{r ref.label=knitr::all_labels()}
#| echo: true
#| eval: false
#| code-fold: true

```
