---
title: "Computational Statistics Coursework 2025/2026"
subtitle: 'MSc in Statistics 2025/26, Imperial College London'
author: "06051710"
format:
  html:
    toc: true
    highlight: tango
    self-contained: true
    df-print: paged
  pdf: default
format-links: false
editor: 
  markdown: 
    wrap: 72
---

```{=html}
<style type="text/css">
h1{
  font-size: 24pt;
}
h2{
  font-size: 18pt;
}
body{
  font-size: 12pt;
}
</style>
```

```{r setup, include = FALSE, tidy=TRUE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
include_solutions <- TRUE
```

```{r setup2, include=FALSE, tidy=TRUE}
set.seed(17)
require(rmarkdown)
require(knitr)
require(kableExtra)
# Put any library imports and other preamble here.
```

I, 06051710, certify that this assessed coursework is my own work,
unless otherwise acknowledged, and includes no plagiarism. I have not
discussed my coursework with anyone else except when seeking
clarification with the module lecturer via email or on MS Teams. I have
not shared any code underlying my coursework with anyone else prior to
submission.

REMEMBER TO ADD A CODEBLOCK THAT ADDS CODE TO THE APPENDIX

# Question 1

**Part a.**\
I implemented the standard version of the Random Walk Metropolis
Hastings algorithm, using 10,000 samples. The first thing I did, mainly
so that I could get a better idea of where to initialise my chain from,
was plot the target distribution, @fig-1.

```{r}
#| echo: false
#| label: fig-1
#| fig-cap: "Target Distribution up to normalisation constant"

#######################
# Question 1
#######################

library(coda)

target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}

t=seq(0,50, length.out=1000)
plot(t, sapply(t, function(t) target(t)), type="l", xlab="x", ylab = "f(x)")
```

The extremely sharp mode at x=20 stood out to me a lot for this
distribution. As such, I decided to initialise the chains from multiple
different starting points, to more easily view the mixing. These
locations were x=2, 15, 20, 25, 30 and 35, as these were close to the 3
modes around x=2, 20, and 30.

After this, I ran the algorithm with standard deviations of
1,2,3,5,8,and 11 in the proposal distribution and plotted the traceplots
for each chain. As can be seen in @fig-2, for sigmas of 1-3 the chains
did not 'mix well', with no chain sampling from all 3 modes. In @fig-3
for sigma=5, it can be seen how some of the chains (specifically the
ones initialised at x=2 and 15), manage to cover all three modes,
however, it seems the distribution is best covered at higher standard
deviations of around 8 and 11. To demonstrate how the higher standard
deviations (8 and 11) were better than that of 5, I created @fig-4 which
shows the histogram of samples generated when a chain is initialised at
x=25. As can be seen, the sigma of 5 completely misses the mode at x=2,
whereas the higher standard deviations sample from it correctly.

```{r}
#| echo: false
#| label: fig-2
#| fig-cap: "Traceplots for smaller standard deviations"


MetrHastw <- function(X0, sigmaprop, nsteps, f){
  X <- numeric(nsteps+1) 
  X[1] <- X0
  for (i in 2:(nsteps+1)){
    Y <- rnorm(1, mean=X[i-1],sd=sigmaprop)
    if (log(runif(1)) <= log(f(Y)) -log(f(X[i-1]))){
      X[i] <- Y}
    else{ X[i] <- X[i-1]}
   }
 X
} 

#seems larger variance is better when we have 10,000 samples for mixing. All chains converge pretty fast, it's just the different modes that make this distribution difficult.

#I should also compute the Gelman Rubin statistic for each chain as a quantitative way to display convergence or lack thereof
#Also group all the plots in one figure

X0s <- c(2, 15, 20, 25, 30, 35)
chains1 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=1, nsteps=10000, f=target)))
chains2 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=2, nsteps=10000, f=target)))
chains3 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=3, nsteps=10000, f=target)))
chains4 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=5, nsteps=10000, f=target)))#actually ends up completely missing the mass around x=20
chains5 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=8, nsteps=10000, f=target)))
chains6 <- lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=11, nsteps=10000, f=target)))

par(mfrow=c(1,3))  

traceplot(chains1); title("sigma = 1")
traceplot(chains2); title("sigma = 2")
traceplot(chains3); title("sigma = 3")
```

```{r}
#| echo: false
#| label: fig-3
#| fig-cap: "Traceplots for larger standard deviations"

par(mfrow=c(1,3))

traceplot(chains4); title("sigma = 5")
traceplot(chains5); title("sigma = 8")
traceplot(chains6); title("sigma = 11")
```

```{r}
#| echo: false
#| label: fig-4
#| fig-cap: "Histograms for different chains initialised at x=25"

par(mfrow=c(1,3))

hist(chains4[[4]], breaks=50, freq=F, xlab='x', main='Sigma=5')
hist(chains5[[4]], breaks=50, freq=F, xlab='x', main='Sigma=8')
hist(chains6[[4]], breaks=50, freq=F, xlab='x', main='Sigma=11')


```

**Part b.**\
The Metropolis-Hasting algorithm I implemented which does not sample
outside the support of $f(x)$ (my target distribution) is an independence sampler, sampling from a
Uniform distribution on [2,45]. I chose 45 as the cutoff point for my distribution as there did not seem to be any probability mass (or anything non negligible) outside of x=45 in my target distribution.

The algorithm works in the following way.

I first initialise my chain with some value $X_0 \in \{2,15,20,25,30,35\}$, since these are all close to the modes of my target.

I then repeat the following for $i=1:N$:

1. Sample $Y \sim U[2,45]$
2. Accept $Y$ as $X_i$, with probability $min(1,f(Y)/f(X_{i-1}))$ (the proposal densities cancel as they are the same for $Y$ and $X_{i-1}$ since they are on a uniform distribution).
3. If $Y$ is not accepted, let $X_i = X_{i-1}$

As shown in @fig-5, all of the chains reached convergence (though there are so many it is hard to see each individually), so specifically focusing on the chain initialised at $X_0 = 25$, the histogram shows this sampler has converged to the true distribution. 

```{r}
#| echo: false
#| label: fig-5
#| fig-cap: "Plots for independence sampler from a Uniform distribution"

### Question 1b

#should probably add a histogram here

target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}


MH_newprop <- function(X0, nsteps, f){
  X <- numeric(nsteps+1) 
  X[1] <- X0
  
  for (i in 2:(nsteps+1)){
    Y <- runif(1, min=2, max=45)
    log_acc <- log(f(Y)) - log(f(X[i-1])) #the proposal densities cancel out as they are the same everywhere 
    
    if (log(runif(1)) <= log_acc){
      X[i] <- Y
    }
    else{X[i] <- X[i-1]}
   }
 X
} 


par(mfrow=c(1,2))

chains_b <- lapply(X0s, function(x0) mcmc(MH_newprop(x0, nsteps=10000, f=target)))
traceplot(chains_b)

hist(chains_b[[4]], breaks=50, freq=F, xlab='x', main='Histogram for X0 = 25')


```

**Part c.**\

```{r}
#| echo: false
#| label: fig-5
#| fig-cap: "Plots for parallel tempering algorithm"

### Question 1c
target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}


paralleltemp <- function(num.steps, Ts, sigmas, g) { #taken from chapter 11 problem set 5.1 solution
  M <- length(Ts)
  samples <- matrix(NA, nrow = num.steps, ncol=M)
  
  X <- rep(3, M) #we start at 3 since we know the target has non zero probability here
  
  for (n in 1:num.steps) {
      # we are going to do a x-step
      for (m in 1:M) {
        y <- X[m] + sigmas[m]*rnorm(1)
        alpha <- g(y)^(1/Ts[m]) / g(X[m])^(1/Ts[m]) 
        
        if (runif(1) <= alpha) {
          X[m] <- y
        }
      }
      ### we are going for a swap
      m <- sample(1:(M-1), 1)
      
      num = (g(X[m])^(1/Ts[m+1])*g(X[m+1])^(1/Ts[m]))
      denom = (g(X[m])^(1/Ts[m])*g(X[m+1])^(1/Ts[m+1]))
      
      if (runif(1) <= num/denom) {
        X[c(m, m+1)] = X[c(m+1, m)]
      }
    
    samples[n, ] = X
  }
  
  return(samples)
}

Ts <- c(1,20,100,200,500) #should probably justify this
num.steps <- 10000
sigmas <- c(0.1,0.5,1,2,5)
samples <- paralleltemp(num.steps, Ts, sigmas, g=target)


par(mfrow=c(1,2))

plot(samples[,1],type="l", main='Traceplot (initialised at 3)')
hist(samples[100:num.steps, 1], breaks=100, freq=F, main='Histogram', xlab='x') 

```









