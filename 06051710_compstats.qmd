---
title: "Computational Statistics Coursework 2025/2026"
subtitle: 'MSc in Statistics 2025/26, Imperial College London'
author: "06051710"
format:
  html:
    toc: true
    highlight: tango
    self-contained: true
    df-print: paged
  pdf: default
format-links: false
editor: 
  markdown: 
    wrap: 72
---

```{=html}
<style type="text/css">
h1{
  font-size: 24pt;
}
h2{
  font-size: 18pt;
}
body{
  font-size: 12pt;
}
</style>
```

```{r setup, include = FALSE, tidy=TRUE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
include_solutions <- TRUE
```

```{r setup2, include=FALSE, tidy=TRUE}
set.seed(17)
require(rmarkdown)
require(knitr)
require(kableExtra)
# Put any library imports and other preamble here.
```

I, 06051710, certify that this assessed coursework is my own work,
unless otherwise acknowledged, and includes no plagiarism. I have not
discussed my coursework with anyone else except when seeking
clarification with the module lecturer via email or on MS Teams. I have
not shared any code underlying my coursework with anyone else prior to
submission.

# Question 1

**Part a.**\
I implemented the standard version of the Random Walk Metropolis
Hastings algorithm, using 10,000 samples. The first thing I did, mainly
so that I could get a better idea of where to initialise my chain from,
was plot the target distribution, @fig-1.

```{r}
#| echo: false
#| label: fig-1
#| fig-cap: "Target Distribution up to normalisation constant"

#######################
# Question 1
#######################

library(coda)

target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}

t=seq(0,50, length.out=1000)
plot(t, sapply(t, function(t) target(t)), type="l", xlab="x", ylab = "f(x)")
```

The extremely sharp mode at x=20 stood out to me a lot for this
distribution. As such, I decided to initialise the chains from multiple
different starting points, to more easily view the mixing. These
locations were x=2, 15, 20, 25, 30 and 35, as these were close to the 3
modes around x=2, 20, and 30.

After this, I ran the algorithm with standard deviations of
1,2,3,5,8,and 11 in the proposal distribution and plotted the traceplots
for each chain. As can be seen in @fig-2, for sigmas of 1-3 the chains
did not 'mix well', with no chain sampling from all 3 modes. In @fig-3
for sigma=5, it can be seen how some of the chains (specifically the
ones initialised at x=2 and 15), manage to cover all three modes,
however, it seems the distribution is best covered at higher standard
deviations of around 8 and 11. To demonstrate how the higher standard
deviations (8 and 11) were better than that of 5, I created @fig-4 which
shows the histogram of samples generated when a chain is initialised at
x=25. As can be seen, the sigma of 5 completely misses the mode at x=2,
whereas the higher standard deviations sample from it correctly.

```{r}
#| echo: false
#| label: fig-2
#| fig-cap: "Traceplots for smaller standard deviations"


MetrHastw <- function(X0, sigmaprop, nsteps, f){
  X <- numeric(nsteps+1) 
  X[1] <- X0
  for (i in 2:(nsteps+1)){
    Y <- rnorm(1, mean=X[i-1],sd=sigmaprop)
    if (log(runif(1)) <= log(f(Y)) -log(f(X[i-1]))){
      X[i] <- Y}
    else{ X[i] <- X[i-1]}
   }
 X
} 

#seems larger variance is better when we have 10,000 samples for mixing. All chains converge pretty fast, it's just the different modes that make this distribution difficult.

#I should also compute the Gelman Rubin statistic for each chain as a quantitative way to display convergence or lack thereof
#Also group all the plots in one figure

X0s <- c(2, 15, 20, 25, 30, 35)
chains1 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=1, nsteps=10000, f=target)))
chains2 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=2, nsteps=10000, f=target)))
chains3 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=3, nsteps=10000, f=target)))
chains4 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=5, nsteps=10000, f=target)))#actually ends up completely missing the mass around x=20
chains5 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=8, nsteps=10000, f=target)))
chains6 <- lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=11, nsteps=10000, f=target)))

par(mfrow=c(1,3))  

traceplot(chains1); title("sigma = 1")
traceplot(chains2); title("sigma = 2")
traceplot(chains3); title("sigma = 3")
```

```{r}
#| echo: false
#| label: fig-3
#| fig-cap: "Traceplots for larger standard deviations"

par(mfrow=c(1,3))

traceplot(chains4); title("sigma = 5")
traceplot(chains5); title("sigma = 8")
traceplot(chains6); title("sigma = 11")
```

```{r}
#| echo: false
#| label: fig-4
#| fig-cap: "Histograms for different chains initialised at x=25"

par(mfrow=c(1,3))

hist(chains4[[4]], breaks=50, freq=F, xlab='x', main='Sigma=5')
hist(chains5[[4]], breaks=50, freq=F, xlab='x', main='Sigma=8')
hist(chains6[[4]], breaks=50, freq=F, xlab='x', main='Sigma=11')


```

**Part b.**\
The Metropolis-Hasting algorithm I implemented which does not sample
outside the support of $f(x)$ (my target distribution) is an
independence sampler, sampling from a Uniform distribution on \[2,45\].
I chose 45 as the cutoff point for my distribution as there did not seem
to be any probability mass (or anything non negligible) outside of x=45
in my target distribution.

The algorithm works in the following way.

I first initialise my chain with some value
$X_0 \in \{2,15,20,25,30,35\}$, since these are all close to the modes
of my target.

I then repeat the following for $i=1:N$:

1.  Sample $Y \sim U[2,45]$
2.  Accept $Y$ as $X_i$, with probability $min(1,f(Y)/f(X_{i-1}))$ (the
    proposal densities cancel as they are the same for $Y$ and $X_{i-1}$
    since they are on a uniform distribution).
3.  If $Y$ is not accepted, let $X_i = X_{i-1}$

As shown in @fig-5, all of the chains reached convergence (though there
are so many it is hard to see each individually), so specifically
focusing on the chain initialised at $X_0 = 25$, the histogram shows
this sampler has converged to the true distribution.

```{r}
#| echo: false
#| label: fig-5
#| fig-cap: "Plots for independence sampler from a Uniform distribution"

### Question 1b

target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}


MH_newprop <- function(X0, nsteps, f){
  X <- numeric(nsteps+1) 
  X[1] <- X0
  
  for (i in 2:(nsteps+1)){
    Y <- runif(1, min=2, max=45)
    log_acc <- log(f(Y)) - log(f(X[i-1])) #the proposal densities cancel out as they are the same everywhere 
    
    if (log(runif(1)) <= log_acc){
      X[i] <- Y
    }
    else{X[i] <- X[i-1]}
   }
 X
} 


par(mfrow=c(1,2))

chains_b <- lapply(X0s, function(x0) mcmc(MH_newprop(x0, nsteps=10000, f=target)))
traceplot(chains_b)

hist(chains_b[[4]], breaks=50, freq=F, xlab='x', main='Histogram for X0 = 25')


```

**Part c.**\

The first thing I did to create my parallel tempering algorithm was
define the temperature schedule my distributions would be based on, seen
in @fig-6. Initially, I used the same temperatures as in the problem set
5.1 solution in chapter 11 of the notes, however, I changed the final
temperature from 500 to 1000, as I felt 500 was still rather steep
around x=20. I also maintained the same range of standard deviations as in the notes , (0.1,0.5,1,2,5), as they seemed to perform fine.

The parallel tempering algorithm then worked as follows (again, assuming
$f$ is the target distribution):

1.  Create a matrix to represent the different chains, with the number
    of rows as the number of samples planned and the number of columns
    as the number of temperatures
2.  Call the most recent vector of values $X$. I initialised all chains to the same value, $x=3$, as the distribution has non zero mass around there (when I tried $x=0$ I ran into problems at first since there is 0 probability mass there). 

Then I did the following loop for $n=1:N$, where $N$ is the number of
samples to be taken from the distribution:

3.  Propose a 'horizontal' step for every chain. That is, within each
    chain, propose and accept/reject a new step using the Random Walk
    Metropolis Hastings algorithm, with standard deviation as specified earlier for each chain.
4.  Sample a number randomly (call it $m$) from $1:M-1$, where $M$ is
    the number of chains. Then accept a swap between the most recent
    sample in chain $m$ and $m+1$ with probability:

$$
\min(1, \frac{ f(X[m])^{\frac{1}{T_s[m+1]}}\, f(X[m+1])^{\frac{1}{T_s[m]}} }
     { f(X[m])^{\frac{1}{T_s[m]}}\, f(X[m+1])^{\frac{1}{T_s[m+1]}} })
$$
After each loop, I updated my matrix recording the samples. After the loop was completed, I returned these samples.

I then plotted the traceplots for the samples from my first chain, as well as a histogram for its density at x=25 (seen in @fig-7). Both the traceplot and the histogram led me to believe my sampler explored the full space and correctly sampled from the target distribution.

```{r}
#| echo: false
#| label: fig-6
#| fig-cap: "Tempered probability distributions, comparing T=1 (pink), T=20 (orange), T=100 (green), T=200 (blue), T=1000 (purple)"

### Question 1c
paralleltemp <- function(num.steps, Ts, sigmas, g, initial) { #taken from chapter 11 problem set 5.1 solution
  M <- length(Ts)
  samples <- matrix(NA, nrow = num.steps, ncol=M)
  
  X <- rep(initial, M) #we start at 3 since we know the target has non zero probability here
  
  for (n in 1:num.steps) {
      # we are going to do a x-step
      for (m in 1:M) {
        y <- X[m] + sigmas[m]*rnorm(1)
        
        #RW Metropolis Hasting Acceptance probability
        alpha <- g(y)^(1/Ts[m]) / g(X[m])^(1/Ts[m])
        
        if (runif(1) <= alpha) {
          X[m] <- y
        }
      }
      ### we are going for a swap
      m <- sample(1:(M-1), 1)
      
      num = (g(X[m])^(1/Ts[m+1])*g(X[m+1])^(1/Ts[m]))
      denom = (g(X[m])^(1/Ts[m])*g(X[m+1])^(1/Ts[m+1]))
      
      if (runif(1) <= num/denom) {
        X[c(m, m+1)] = X[c(m+1, m)]
      }
    
    samples[n, ] = X
  }
  
  return(samples)
}

targetvector <- function(x) {
  ifelse(
    x >= 2,
    exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01),
    0
  )
}

Ts <- c(1,20,100,200,1000) 
#can plot the tempered distributions

x <- seq(0, 50, length.out = 500)

# empty plot
plot(NA, xlim = c(0, 50), ylim = c(0, 1), 
     xlab = "x", ylab = "density")

# colour palette
cols <- hcl.colors(length(Ts), "Set2")

# add lines
for (i in seq_along(Ts)) {
  T <- Ts[i]
  lines(x, targetvector(x)^(1/T), col = cols[i], lwd = 2)
}
```

```{r}
#| echo: false
#| label: fig-7
#| fig-cap: "Plots for parallel tempering algorithm"
num.steps <- 10000
sigmas <- c(0.1,0.5,1,2,5)
X0s <- c(2, 15, 20, 25, 30, 35)

justchain1 <- function(x0) {
  return(paralleltemp(num.steps, Ts, sigmas, g=target, initial=x0)[,1])
}

chains_parallel <-lapply(X0s, function(x0) mcmc(justchain1(x0)))


par(mfrow=c(1,2))

traceplot(chains_parallel); title("Traceplot for Parallel tempering")
hist(chains_parallel[[4]], breaks=100, freq=F, main='Histogram for X0=25 ', xlab='x') 

```





# Code Appendix {#sec-code-appendix}

```{r ref.label=knitr::all_labels()}
#| echo: true
#| eval: false
#| code-fold: true

```
