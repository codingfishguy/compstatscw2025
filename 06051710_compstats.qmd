---
title: "Computational Statistics Coursework 2025/2026"
subtitle: 'MSc in Statistics 2025/26, Imperial College London'
author: "06051710"
format:
  html:
    toc: true
    highlight: tango
    self-contained: true
    df-print: paged
  pdf: default
format-links: false
editor: 
  markdown: 
    wrap: 72
---

```{=html}
<style type="text/css">
h1{
  font-size: 24pt;
}
h2{
  font-size: 18pt;
}
body{
  font-size: 12pt;
}
</style>
```

```{r setup, include = FALSE, tidy=TRUE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
include_solutions <- TRUE
```

```{r setup2, include=FALSE, tidy=TRUE}
set.seed(17)
require(rmarkdown)
require(knitr)
require(kableExtra)
# Put any library imports and other preamble here.
```

I, 06051710, certify that this assessed coursework is my own work,
unless otherwise acknowledged, and includes no plagiarism. I have not
discussed my coursework with anyone else except when seeking
clarification with the module lecturer via email or on MS Teams. I have
not shared any code underlying my coursework with anyone else prior to
submission.

# Question 1

Throughout this question, I evaluated each sampling algorithm with 1000
samples. I found this made comparing algorithms easier, especially since
the traceplots were less dense.

The first thing I did, mainly so that I could get a better idea of where
to initialise my chains from, was plot the target distribution, @fig-1.

```{r}
#| echo: false
#| label: fig-1
#| fig-cap: "Target Distribution up to normalisation constant"

#######################
# Question 1
#######################

library(coda)

target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}

t=seq(0,50, length.out=1000)
plot(t, sapply(t, function(t) target(t)), type="l", xlab="x", ylab = "f(x)")
```

The extremely sharp mode at $x=20$ stood out to me a lot for this
distribution. As such, I decided to initialise the chains from multiple
different starting points, to more easily view the mixing. These
locations were $x=2, 15, 20, 25, 30, 35$, as these were close to the 3
modes around $x$ =2, 20, and 30.

### Part a

When implementing the Random Walk Metropolis Hastings algorithm, I ran
the algorithm separately for standard deviations of 1,3,5,9,11 and 15 in
the proposal distribution, to investigate the impact of different
variances on convergence and mixing quality of my chains.

Visually, I inspected convergence using traceplots in @fig-2 and @fig-3.
As can be seen in @fig-2, for $\sigma$ of 1-5 the chains did not mix
well at all, with no chain sampling from all 3 modes. Convergence seems
best at higher standard deviations; in @fig-3 it is only with a standard
deviation of 11 that it seems most of the modes are sampled from.

```{r}
#| echo: false
#| label: fig-2
#| fig-cap: "Traceplots for smaller standard deviations"


MetrHastw <- function(X0, sigmaprop, nsteps, f){
  X <- numeric(nsteps+1) 
  X[1] <- X0
  for (i in 2:(nsteps+1)){
    Y <- rnorm(1, mean=X[i-1],sd=sigmaprop)
    if (log(runif(1)) <= log(f(Y)) -log(f(X[i-1]))){
      X[i] <- Y}
    else{ X[i] <- X[i-1]}
   }
 X[-(1:5)] #discarding burn in
} 

#seems larger variance is better when we have 10,000 samples for mixing. All chains converge pretty fast, it's just the different modes that make this distribution difficult.

#I should also compute the Gelman Rubin statistic for each chain as a quantitative way to display convergence or lack thereof
#Also group all the plots in one figure

X0s <- c(2, 15, 20, 25, 30, 35)
chains1 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=1, nsteps=1000, f=target)))
chains2 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=3, nsteps=1000, f=target)))
chains3 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=5, nsteps=1000, f=target)))
chains4 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=9, nsteps=1000, f=target)))#actually ends up completely missing the mass around x=20
chains5 <-lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=11, nsteps=1000, f=target)))
chains6 <- lapply(X0s, function(x0) mcmc(MetrHastw(x0, sigmaprop=15, nsteps=1000, f=target)))

par(mfrow=c(1,3))  

traceplot(chains1); title("sigma = 1")
traceplot(chains2); title("sigma = 3")
traceplot(chains3); title("sigma = 5")
```

```{r}
#| echo: false
#| label: fig-3
#| fig-cap: "Traceplots for larger standard deviations"

par(mfrow=c(1,3))

traceplot(chains4); title("sigma = 9")
traceplot(chains5); title("sigma = 11")
traceplot(chains6); title("sigma = 15")
```

To get a numerical assessment of convergence, I also calculated the
Gelman-Rubin test statistic for each standard deviation.

```{r}
#| echo: false


chains <- list(chains1, chains2, chains3, chains4, chains5, chains6)

res <- data.frame(
  PointEst = sapply(chains, function(x) gelman.diag(x)$psrf[1]),
  UpperCI  = sapply(chains, function(x) gelman.diag(x)$psrf[2])
)

rownames(res) <- c("Sigma 1", "Sigma 3", "Sigma 5", "Sigma 9", "Sigma 11", "Sigma 15")
res
```

Using the common heuristic approach that $\sqrt{\hat{R}} \leq 1.1$ we
find that for standard deviations of 11 and above, the chains likely
converged to the same distribution, however, it seems a standard
deviation of 15 is best, as even the upper confidence interval value
would pass this heuristic test (whereas the same cannot be said for a
standard deviation of 11).

### Part b

The Metropolis-Hasting algorithm I implemented which does not sample
outside the support of $f(x)$ (my target distribution) is an
independence sampler, sampling from a Uniform distribution on \[2,45\].
I chose 45 as the cutoff point for my distribution as there did not seem
to be any probability mass (or anything non negligible) outside of x=45
in my target distribution.

The algorithm works in the following way.

I first initialise my chain with some value
$X_0 \in \{2,15,20,25,30,35\}$, since these are all close to the modes
of my target.

I then repeat the following for $i=1:N$:

1.  Sample $Y \sim U[2,45]$
2.  Accept $Y$ as $X_i$, with probability $min(1,f(Y)/f(X_{i-1}))$ (the
    proposal densities cancel as they are the same for $Y$ and $X_{i-1}$
    since they are on a uniform distribution).
3.  If $Y$ is not accepted, let $X_i = X_{i-1}$

```{r}
#| echo: false
#| label: fig-5
#| fig-cap: "Plots for independence sampler from a Uniform distribution"

### Question 1b

target <- function(x) {
  if (x>=2){
    return (exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01))
  }
  else{
    return (0)
  }
}


MH_newprop <- function(X0, nsteps, f){
  X <- numeric(nsteps+1) 
  X[1] <- X0
  
  for (i in 2:(nsteps+1)){
    Y <- runif(1, min=2, max=45)
    log_acc <- log(f(Y)) - log(f(X[i-1])) #the proposal densities cancel out as they are the same everywhere 
    
    if (log(runif(1)) <= log_acc){
      X[i] <- Y
    }
    else{X[i] <- X[i-1]}
   }
 X[-(1:5)]
} 



chains_b <- lapply(X0s, function(x0) mcmc(MH_newprop(x0, nsteps=1000, f=target))) #independence sampler is SUPER good with only 1000 iterations!!!
traceplot(chains_b)



```

As shown in @fig-5, graphically, it seems that the chain has converged
to the true distribution. The Gelman Rubin statistic (below) further
confirms this.

```{r}
#| echo: false
gelman.diag(chains_b)
```

### Part c

The first thing I did to create my parallel tempering algorithm was
define the temperature schedule my distributions would be based on, seen
in @fig-6. Initially, I used the same temperatures as in the solution to
problem set 5.1 in chapter 11 of the notes, however, I changed the final
temperature from 500 to 1000, as I felt 500 was still rather steep
around x=20. I also maintained the same range of standard deviations as
in the notes, (0.1,0.5,1,2,5), as I found no reason to alter them (they
seemed to lead to a fine performance).

In pseudo-code my algorithm worked as follows (with $g$ as the target
distribution):

$$
\begin{array}{l}
\text{REPEAT FOR NUMBER_OF_SAMPLES:} \\
\quad \text{FOR EACH CHAIN M:} \\
\qquad \text{PROPOSE} \quad Y = \text{CURRENT_STATE} + N(0, \sigma[M]) \quad \# \text{RWMH update} \\
\qquad \text{ACCEPT} \quad Y \text{ AS CURRENT_STATE IF:} \\
\qquad \quad u \sim \mathcal{U}(0,1) \\
\qquad \quad u \le \dfrac{(g(Y))^{1/\text{TempM}}}{(g(\text{CURRENT_STATE}))^{1/\text{TempM}}} \\
\\
\text{SAMPLE} \quad m \in (1:M-1) \\
\text{FOR CURRENT_STATES IN CHAIN}(m) \text{ AND CHAIN}(m+1): \\
\text{SWAP STATES IF:} \\
\quad u \sim \mathcal{U}(0,1) \\
\quad u \le \dfrac{(g(\text{STATE}(m)))^{1/\text{Temp}(m+1)} \cdot (g(\text{STATE}(m+1)))^{1/\text{Temp}(m)}}{(g(\text{STATE}m))^{1/\text{Temp}(m)} \cdot (g(\text{STATE}(m+1)))^{1/\text{Temp}(m+1)}}
\end{array}
$$

The parallel tempering algorithm then worked as follows (again, assuming
$f$ is the target distribution):

1.  Create a matrix to represent the different chains, with the number
    of rows as the number of samples planned and the number of columns
    as the number of temperatures.
2.  Call the most recent vector of values $X$. Initialise all chains to
    the same value.

Then I did the following loop for $n=1:N$, where $N$ is the number of
samples to be taken from the distribution:

3.  Perform a 'horizontal' step for every chain. That is, within each
    chain, propose and accept/reject a new step using the Random Walk
    Metropolis Hastings algorithm, with standard deviation as specified
    earlier for each chain.
4.  Sample a number randomly (call it $m$) from $1:M-1$, where $M$ is
    the number of chains. Then accept a swap between the most recent
    sample in chain $m$ and $m+1$ with probability:

$$
\min(1, \frac{ f(X[m])^{\frac{1}{T_s[m+1]}}\, f(X[m+1])^{\frac{1}{T_s[m]}} }
     { f(X[m])^{\frac{1}{T_s[m]}}\, f(X[m+1])^{\frac{1}{T_s[m+1]}} })
$$ After each loop, I updated my matrix recording the samples. After the
loop was completed, I returned these samples.

I then plotted the traceplots for the samples from my first chain, as
well as a histogram for its density at x=25 (seen in @fig-7). Both the
traceplot and the histogram led me to believe my sampler explored the
full space well and correctly sampled from the target distribution.

```{r}
#| echo: false
#| label: fig-6
#| fig-cap: "Tempered probability distributions, comparing T=1 (pink), T=20 (orange), T=100 (green), T=200 (blue), T=1000 (purple)"

### Question 1c
paralleltemp <- function(num.steps, Ts, sigmas, g, initial) { #taken from chapter 11 problem set 5.1 solution
  M <- length(Ts)
  samples <- matrix(NA, nrow = num.steps, ncol=M)
  
  X <- rep(initial, M) #we start at 3 since we know the target has non zero probability here
  
  for (n in 1:num.steps) {
      # we are going to do a x-step
      for (m in 1:M) {
        y <- X[m] + sigmas[m]*rnorm(1)
        
        #RW Metropolis Hasting Acceptance probability
        alpha <- g(y)^(1/Ts[m]) / g(X[m])^(1/Ts[m])
        
        if (runif(1) <= alpha) {
          X[m] <- y
        }
      }
      ### we are going for a swap
      m <- sample(1:(M-1), 1)
      
      num = (g(X[m])^(1/Ts[m+1])*g(X[m+1])^(1/Ts[m]))
      denom = (g(X[m])^(1/Ts[m])*g(X[m+1])^(1/Ts[m+1]))
      
      if (runif(1) <= num/denom) {
        X[c(m, m+1)] = X[c(m+1, m)]
      }
    
    samples[n, ] = X
  }
  
  return(samples) #should probably discard some burn in
}

targetvector <- function(x) {
  ifelse(
    x >= 2,
    exp(-3*(x-2)) + exp(-(x-30)^2) + exp(-((x-20)^2)/0.01),
    0
  )
}

Ts <- c(1,20,100,200,1000) 
#can plot the tempered distributions

x <- seq(0, 50, length.out = 500)

# empty plot
plot(NA, xlim = c(0, 50), ylim = c(0, 1), 
     xlab = "x", ylab = "density")

# colour palette
cols <- hcl.colors(length(Ts), "Set2")

# add lines
for (i in seq_along(Ts)) {
  T <- Ts[i]
  lines(x, targetvector(x)^(1/T), col = cols[i], lwd = 2)
}
```

```{r}
#| echo: false
#| label: fig-7
#| fig-cap: "Plots for parallel tempering algorithm"
num.steps <- 1000 
sigmas <- c(0.1,0.5,1,2,5)
X0s <- c(2, 15, 20, 25, 30, 35)

justchain1 <- function(x0) {
  return(paralleltemp(num.steps, Ts, sigmas, g=target, initial=x0)[,1])
}

chains_parallel <-lapply(X0s, function(x0) mcmc(justchain1(x0)))


par(mfrow=c(1,2))

traceplot(chains_parallel); title("Traceplot for Parallel tempering")
hist(chains_parallel[[4]], breaks=100, freq=F, main='Histogram for X0=25 ', xlab='x') 

```

### Part d

# Code Appendix {#sec-code-appendix}

```{r ref.label=knitr::all_labels()}
#| echo: true
#| eval: false
#| code-fold: true

```
